(u'correspondence', u'0')
(u'init_ckpt', u'/srv/glusterfs/xieya/tf_224_1/models/model.ckpt-239000')
(u'g_repeat', u'1')
(u'is_coco', u'1')
(u'restore_opt', u'1')
(u'with_caption', u'0')
(u'd_repeat', u'1')
(u'prior_boost', u'1')
(u'is_gan', u'0')
(u'batch_size', u'32')
(u'gan', u'0')
(u'is_rgb', u'0')
(u'image_size', u'224')
(u'gpus', u'0')
(u'gp_lambda', u'10')
(u'k', u'1')
(u'g_version', u'1')
(u'version', u'11')
(u'temp_trainable', u'0')
(u'alpha', u'0.01')
(u'weight_decay', u'0.001')
Discriminator has no correspondence.
Not using GAN.
Using prior boost.
Learning rate G: 4e-06 D: 0.0001
Adversarial weight 0.01
Generator version 1
Discriminator version 11
Gradient penalty 10.0.
Gradient norm 1.0.
Solver initialization done.
Graph constructed.
Session configured.
Initialized.
Init generator with /srv/glusterfs/xieya/tf_224_1/models/model.ckpt-239000.
2018-10-09 14:47:31.216489: step 0, G loss = 10999.21, new loss = -38.21(36.0 examples/sec; 0.888 sec/batch)
2018-10-09 14:47:50.250592: step 10, G loss = 10707.02, new loss = 313.28(17.3 examples/sec; 1.852 sec/batch)
2018-10-09 14:47:58.334751: step 20, G loss = 10592.20, new loss = -204.87(41.5 examples/sec; 0.770 sec/batch)
2018-10-09 14:48:06.259372: step 30, G loss = 10177.10, new loss = -120.00(42.9 examples/sec; 0.747 sec/batch)
2018-10-09 14:48:14.255335: step 40, G loss = 10473.43, new loss = 612.09(42.3 examples/sec; 0.756 sec/batch)
2018-10-09 14:48:22.357061: step 50, G loss = 10671.91, new loss = 34.18(42.3 examples/sec; 0.756 sec/batch)
2018-10-09 14:48:30.494232: step 60, G loss = 10673.50, new loss = -183.50(41.6 examples/sec; 0.769 sec/batch)
2018-10-09 14:48:38.566798: step 70, G loss = 10272.39, new loss = -970.14(42.3 examples/sec; 0.757 sec/batch)
2018-10-09 14:48:46.256041: step 80, G loss = 10753.46, new loss = 1798.89(44.1 examples/sec; 0.726 sec/batch)
2018-10-09 14:48:53.735572: step 90, G loss = 10631.56, new loss = 75.82(45.5 examples/sec; 0.703 sec/batch)
2018-10-09 14:49:00.761061: step 100, G loss = 10493.18, new loss = -18.94(48.1 examples/sec; 0.665 sec/batch)
2018-10-09 14:49:10.329491: step 110, G loss = 10833.57, new loss = 588.63(35.0 examples/sec; 0.915 sec/batch)
2018-10-09 14:49:18.647071: step 120, G loss = 10700.05, new loss = 180.58(41.2 examples/sec; 0.777 sec/batch)
2018-10-09 14:49:26.207734: step 130, G loss = 10635.18, new loss = -238.63(44.6 examples/sec; 0.718 sec/batch)
2018-10-09 14:49:33.556294: step 140, G loss = 10513.90, new loss = -460.98(45.9 examples/sec; 0.697 sec/batch)
2018-10-09 14:49:40.928517: step 150, G loss = 10808.44, new loss = -7.37(45.8 examples/sec; 0.698 sec/batch)
2018-10-09 14:49:48.404363: step 160, G loss = 10197.02, new loss = -303.50(45.1 examples/sec; 0.710 sec/batch)
2018-10-09 14:49:55.952943: step 170, G loss = 10507.25, new loss = -402.94(44.7 examples/sec; 0.716 sec/batch)
2018-10-09 14:50:01.790310: step 180, G loss = 10264.11, new loss = -288.04(57.3 examples/sec; 0.558 sec/batch)
2018-10-09 14:50:07.638898: step 190, G loss = 10257.68, new loss = -251.08(57.1 examples/sec; 0.560 sec/batch)
2018-10-09 14:50:13.485507: step 200, G loss = 10276.51, new loss = -691.06(57.1 examples/sec; 0.560 sec/batch)
2018-10-09 14:50:19.925214: step 210, G loss = 10618.98, new loss = -230.08(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 14:50:25.765732: step 220, G loss = 10627.04, new loss = -443.54(57.2 examples/sec; 0.559 sec/batch)
2018-10-09 14:50:31.671544: step 230, G loss = 10562.41, new loss = -315.22(56.5 examples/sec; 0.566 sec/batch)
2018-10-09 14:50:37.516705: step 240, G loss = 10020.41, new loss = -497.82(57.1 examples/sec; 0.560 sec/batch)
2018-10-09 14:50:43.367867: step 250, G loss = 10692.17, new loss = 38.34(57.1 examples/sec; 0.560 sec/batch)
2018-10-09 14:50:49.236643: step 260, G loss = 10616.38, new loss = -164.66(56.9 examples/sec; 0.562 sec/batch)
2018-10-09 14:50:55.107404: step 270, G loss = 10790.14, new loss = -194.57(57.1 examples/sec; 0.560 sec/batch)
2018-10-09 14:51:00.892867: step 280, G loss = 10054.37, new loss = 113.84(57.8 examples/sec; 0.554 sec/batch)
2018-10-09 14:51:06.760768: step 290, G loss = 10869.83, new loss = 196.55(56.9 examples/sec; 0.562 sec/batch)
2018-10-09 14:51:12.562335: step 300, G loss = 10369.84, new loss = -532.60(57.6 examples/sec; 0.556 sec/batch)
2018-10-09 14:51:19.781364: step 310, G loss = 10905.81, new loss = -136.72(45.9 examples/sec; 0.697 sec/batch)
2018-10-09 14:51:25.657141: step 320, G loss = 10884.29, new loss = 838.58(56.8 examples/sec; 0.563 sec/batch)
2018-10-09 14:51:31.561743: step 330, G loss = 10483.42, new loss = -13.99(56.6 examples/sec; 0.566 sec/batch)
2018-10-09 14:51:37.457385: step 340, G loss = 9982.00, new loss = -472.08(56.6 examples/sec; 0.565 sec/batch)
2018-10-09 14:51:43.363745: step 350, G loss = 11097.58, new loss = 1177.81(56.6 examples/sec; 0.565 sec/batch)
2018-10-09 14:51:49.232008: step 360, G loss = 10600.84, new loss = 58.42(56.9 examples/sec; 0.563 sec/batch)
2018-10-09 14:51:55.032147: step 370, G loss = 10846.89, new loss = 871.42(57.6 examples/sec; 0.556 sec/batch)
2018-10-09 14:52:00.881151: step 380, G loss = 10078.32, new loss = -453.40(57.0 examples/sec; 0.561 sec/batch)
2018-10-09 14:52:06.725555: step 390, G loss = 10128.09, new loss = -316.84(57.3 examples/sec; 0.559 sec/batch)
2018-10-09 14:52:12.545000: step 400, G loss = 10243.24, new loss = -281.77(57.3 examples/sec; 0.558 sec/batch)
2018-10-09 14:52:18.982625: step 410, G loss = 10582.34, new loss = 34.83(51.7 examples/sec; 0.619 sec/batch)
2018-10-09 14:52:24.767712: step 420, G loss = 10920.87, new loss = 297.91(57.7 examples/sec; 0.554 sec/batch)
2018-10-09 14:52:30.496226: step 430, G loss = 10814.03, new loss = 82.33(58.3 examples/sec; 0.549 sec/batch)
2018-10-09 14:52:36.238827: step 440, G loss = 10131.14, new loss = -729.16(58.2 examples/sec; 0.550 sec/batch)
2018-10-09 14:52:42.619320: step 450, G loss = 10749.17, new loss = 215.81(52.8 examples/sec; 0.606 sec/batch)
2018-10-09 14:52:49.304008: step 460, G loss = 10605.67, new loss = -183.40(50.4 examples/sec; 0.636 sec/batch)
2018-10-09 14:52:56.223667: step 470, G loss = 10568.06, new loss = -355.19(48.5 examples/sec; 0.660 sec/batch)
2018-10-09 14:53:02.821222: step 480, G loss = 10709.56, new loss = 939.78(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 14:53:09.490043: step 490, G loss = 9956.10, new loss = -779.00(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 14:53:16.120404: step 500, G loss = 10833.76, new loss = 87.70(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 14:53:23.538141: step 510, G loss = 10088.64, new loss = -766.01(45.0 examples/sec; 0.711 sec/batch)
2018-10-09 14:53:30.193204: step 520, G loss = 10684.60, new loss = -129.66(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 14:53:36.820380: step 530, G loss = 10975.96, new loss = -85.48(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 14:53:43.482929: step 540, G loss = 10498.51, new loss = 377.16(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 14:53:50.115819: step 550, G loss = 10744.25, new loss = 977.71(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 14:53:56.851783: step 560, G loss = 10422.01, new loss = 215.44(49.9 examples/sec; 0.641 sec/batch)
2018-10-09 14:54:03.507949: step 570, G loss = 10370.29, new loss = 86.19(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 14:54:10.326445: step 580, G loss = 10423.65, new loss = 85.99(49.4 examples/sec; 0.648 sec/batch)
2018-10-09 14:54:17.023480: step 590, G loss = 10670.12, new loss = 532.07(50.1 examples/sec; 0.639 sec/batch)
2018-10-09 14:54:23.732493: step 600, G loss = 10377.30, new loss = -194.21(50.1 examples/sec; 0.638 sec/batch)
2018-10-09 14:54:31.172772: step 610, G loss = 11119.66, new loss = 166.60(44.9 examples/sec; 0.713 sec/batch)
2018-10-09 14:54:37.792790: step 620, G loss = 10617.99, new loss = -455.18(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 14:54:44.458098: step 630, G loss = 11130.51, new loss = 143.47(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 14:54:51.136093: step 640, G loss = 10312.07, new loss = -350.50(50.3 examples/sec; 0.637 sec/batch)
2018-10-09 14:54:57.687366: step 650, G loss = 10481.41, new loss = -590.73(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 14:55:04.339955: step 660, G loss = 10414.29, new loss = -181.97(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 14:55:11.044827: step 670, G loss = 10311.18, new loss = -407.83(50.1 examples/sec; 0.639 sec/batch)
2018-10-09 14:55:17.656662: step 680, G loss = 10374.07, new loss = 74.74(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 14:55:24.409570: step 690, G loss = 10417.76, new loss = -193.12(49.6 examples/sec; 0.645 sec/batch)
2018-10-09 14:55:31.067577: step 700, G loss = 10874.31, new loss = 12.54(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 14:55:38.517416: step 710, G loss = 10283.02, new loss = -193.09(44.9 examples/sec; 0.712 sec/batch)
2018-10-09 14:55:45.171672: step 720, G loss = 10620.04, new loss = -122.37(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 14:55:51.909082: step 730, G loss = 10238.73, new loss = 191.29(50.0 examples/sec; 0.640 sec/batch)
2018-10-09 14:55:58.503650: step 740, G loss = 10593.87, new loss = -142.19(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 14:56:05.226250: step 750, G loss = 10600.70, new loss = -796.82(50.0 examples/sec; 0.640 sec/batch)
2018-10-09 14:56:11.986811: step 760, G loss = 10511.15, new loss = -339.73(49.9 examples/sec; 0.642 sec/batch)
2018-10-09 14:56:18.686791: step 770, G loss = 10833.14, new loss = 462.66(50.3 examples/sec; 0.636 sec/batch)
2018-10-09 14:56:25.547887: step 780, G loss = 10351.39, new loss = -516.63(49.0 examples/sec; 0.653 sec/batch)
2018-10-09 14:56:32.193182: step 790, G loss = 10502.83, new loss = 229.10(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 14:56:38.895676: step 800, G loss = 10089.98, new loss = -132.46(50.2 examples/sec; 0.638 sec/batch)
2018-10-09 14:56:46.392603: step 810, G loss = 10221.03, new loss = -87.71(44.6 examples/sec; 0.717 sec/batch)
2018-10-09 14:56:53.077523: step 820, G loss = 10485.51, new loss = 676.07(50.3 examples/sec; 0.636 sec/batch)
2018-10-09 14:56:59.788708: step 830, G loss = 10248.17, new loss = -126.65(50.1 examples/sec; 0.638 sec/batch)
2018-10-09 14:57:06.540819: step 840, G loss = 9999.83, new loss = -603.22(49.9 examples/sec; 0.642 sec/batch)
2018-10-09 14:57:13.356896: step 850, G loss = 10755.45, new loss = -161.92(49.2 examples/sec; 0.650 sec/batch)
2018-10-09 14:57:20.080281: step 860, G loss = 10585.68, new loss = -142.48(50.0 examples/sec; 0.640 sec/batch)
2018-10-09 14:57:26.732173: step 870, G loss = 10460.24, new loss = -330.68(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 14:57:33.447482: step 880, G loss = 11299.85, new loss = 2180.17(50.1 examples/sec; 0.639 sec/batch)
2018-10-09 14:57:40.044795: step 890, G loss = 10130.29, new loss = -356.23(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 14:57:46.704405: step 900, G loss = 10401.38, new loss = 22.04(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 14:57:54.186890: step 910, G loss = 10374.41, new loss = 126.87(44.7 examples/sec; 0.715 sec/batch)
2018-10-09 14:58:00.915162: step 920, G loss = 10695.74, new loss = 343.98(49.9 examples/sec; 0.641 sec/batch)
2018-10-09 14:58:07.606589: step 930, G loss = 10430.83, new loss = -249.67(50.3 examples/sec; 0.636 sec/batch)
2018-10-09 14:58:14.271226: step 940, G loss = 10771.42, new loss = 80.29(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 14:58:20.957798: step 950, G loss = 10486.38, new loss = 40.43(50.4 examples/sec; 0.634 sec/batch)
2018-10-09 14:58:27.689571: step 960, G loss = 11026.83, new loss = 790.48(49.9 examples/sec; 0.641 sec/batch)
2018-10-09 14:58:34.356882: step 970, G loss = 10754.31, new loss = 1057.78(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 14:58:40.966874: step 980, G loss = 10296.91, new loss = 446.28(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 14:58:47.633740: step 990, G loss = 10411.92, new loss = -133.58(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 14:58:54.351274: step 1000, G loss = 10575.63, new loss = -459.47(50.1 examples/sec; 0.639 sec/batch)
2018-10-09 14:59:07.093849: step 1010, G loss = 10527.18, new loss = 533.20(25.8 examples/sec; 1.240 sec/batch)
2018-10-09 14:59:13.852520: step 1020, G loss = 10562.22, new loss = -486.78(49.7 examples/sec; 0.644 sec/batch)
2018-10-09 14:59:20.487964: step 1030, G loss = 10563.89, new loss = 146.44(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 14:59:27.156105: step 1040, G loss = 10175.42, new loss = -232.84(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 14:59:33.854195: step 1050, G loss = 10435.17, new loss = 202.72(50.2 examples/sec; 0.637 sec/batch)
2018-10-09 14:59:40.850104: step 1060, G loss = 10631.58, new loss = -397.55(48.0 examples/sec; 0.666 sec/batch)
2018-10-09 14:59:47.608192: step 1070, G loss = 10335.80, new loss = 54.56(49.8 examples/sec; 0.642 sec/batch)
2018-10-09 14:59:54.328470: step 1080, G loss = 10203.54, new loss = -95.15(50.1 examples/sec; 0.639 sec/batch)
2018-10-09 15:00:01.013686: step 1090, G loss = 10057.70, new loss = -800.46(50.3 examples/sec; 0.636 sec/batch)
2018-10-09 15:00:07.676647: step 1100, G loss = 10655.62, new loss = 154.87(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 15:00:15.100163: step 1110, G loss = 10666.53, new loss = 652.55(45.1 examples/sec; 0.710 sec/batch)
2018-10-09 15:00:21.683710: step 1120, G loss = 10776.31, new loss = -445.22(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:00:28.297262: step 1130, G loss = 10473.21, new loss = -280.07(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:00:34.931910: step 1140, G loss = 10508.82, new loss = 113.72(50.7 examples/sec; 0.632 sec/batch)
2018-10-09 15:00:41.645083: step 1150, G loss = 10657.85, new loss = 352.93(50.1 examples/sec; 0.638 sec/batch)
2018-10-09 15:00:48.278649: step 1160, G loss = 10452.31, new loss = -112.46(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:00:55.007538: step 1170, G loss = 10533.21, new loss = 63.61(50.0 examples/sec; 0.640 sec/batch)
2018-10-09 15:01:01.719702: step 1180, G loss = 10557.29, new loss = -75.18(50.1 examples/sec; 0.639 sec/batch)
2018-10-09 15:01:08.347507: step 1190, G loss = 10347.23, new loss = -355.88(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:01:14.977672: step 1200, G loss = 10316.17, new loss = -16.66(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:01:22.313105: step 1210, G loss = 10446.90, new loss = -190.01(45.7 examples/sec; 0.701 sec/batch)
2018-10-09 15:01:28.973262: step 1220, G loss = 10382.98, new loss = 400.22(50.4 examples/sec; 0.634 sec/batch)
2018-10-09 15:01:35.613517: step 1230, G loss = 10358.63, new loss = -498.45(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:01:42.590653: step 1240, G loss = 10879.35, new loss = 693.31(48.2 examples/sec; 0.665 sec/batch)
2018-10-09 15:01:49.240313: step 1250, G loss = 10283.30, new loss = 416.09(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:01:55.889095: step 1260, G loss = 10806.96, new loss = 425.53(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:02:02.526414: step 1270, G loss = 10575.98, new loss = -210.91(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:02:09.179639: step 1280, G loss = 10768.33, new loss = 757.67(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:02:15.826089: step 1290, G loss = 10340.65, new loss = 464.01(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:02:22.414164: step 1300, G loss = 10534.13, new loss = 0.98(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:02:29.856237: step 1310, G loss = 10922.80, new loss = 690.70(44.9 examples/sec; 0.712 sec/batch)
2018-10-09 15:02:36.632112: step 1320, G loss = 10655.38, new loss = 109.86(49.7 examples/sec; 0.644 sec/batch)
2018-10-09 15:02:43.529064: step 1330, G loss = 10815.87, new loss = 65.94(48.7 examples/sec; 0.657 sec/batch)
2018-10-09 15:02:50.141433: step 1340, G loss = 10663.50, new loss = 1377.91(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:02:56.831441: step 1350, G loss = 10751.65, new loss = 917.57(50.3 examples/sec; 0.637 sec/batch)
2018-10-09 15:03:03.540220: step 1360, G loss = 10762.38, new loss = 628.70(50.2 examples/sec; 0.638 sec/batch)
2018-10-09 15:03:10.180795: step 1370, G loss = 10265.57, new loss = -576.75(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:03:16.927743: step 1380, G loss = 10734.58, new loss = -143.32(49.9 examples/sec; 0.642 sec/batch)
2018-10-09 15:03:23.550720: step 1390, G loss = 10495.83, new loss = 288.85(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:03:30.244801: step 1400, G loss = 10510.99, new loss = 502.64(50.3 examples/sec; 0.636 sec/batch)
2018-10-09 15:03:37.913813: step 1410, G loss = 10738.65, new loss = 236.94(43.6 examples/sec; 0.734 sec/batch)
2018-10-09 15:03:44.544927: step 1420, G loss = 10385.51, new loss = -355.45(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:03:51.254996: step 1430, G loss = 10442.94, new loss = -285.46(50.2 examples/sec; 0.637 sec/batch)
2018-10-09 15:03:57.907867: step 1440, G loss = 10264.30, new loss = 2.04(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:04:04.612750: step 1450, G loss = 10793.78, new loss = -209.48(50.2 examples/sec; 0.637 sec/batch)
2018-10-09 15:04:11.339852: step 1460, G loss = 10714.15, new loss = 321.44(50.0 examples/sec; 0.640 sec/batch)
2018-10-09 15:04:18.007416: step 1470, G loss = 10743.20, new loss = -28.68(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 15:04:24.759248: step 1480, G loss = 10578.31, new loss = -373.99(49.8 examples/sec; 0.642 sec/batch)
2018-10-09 15:04:31.438169: step 1490, G loss = 10208.21, new loss = -675.80(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 15:04:38.094806: step 1500, G loss = 10364.82, new loss = -501.60(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:04:45.591477: step 1510, G loss = 10240.29, new loss = -437.10(44.7 examples/sec; 0.716 sec/batch)
2018-10-09 15:04:52.210925: step 1520, G loss = 10568.95, new loss = -88.87(50.8 examples/sec; 0.629 sec/batch)
2018-10-09 15:04:58.865064: step 1530, G loss = 11092.42, new loss = -64.93(50.7 examples/sec; 0.632 sec/batch)
2018-10-09 15:05:05.521923: step 1540, G loss = 10403.68, new loss = -552.11(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:05:12.234093: step 1550, G loss = 10659.39, new loss = 673.02(50.1 examples/sec; 0.638 sec/batch)
2018-10-09 15:05:18.894866: step 1560, G loss = 10724.43, new loss = 359.69(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 15:05:25.518740: step 1570, G loss = 10447.06, new loss = 584.02(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:05:32.184914: step 1580, G loss = 10875.24, new loss = 271.13(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 15:05:38.840552: step 1590, G loss = 10559.22, new loss = -128.80(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:05:45.753509: step 1600, G loss = 10818.83, new loss = -328.67(48.7 examples/sec; 0.657 sec/batch)
2018-10-09 15:05:53.177701: step 1610, G loss = 10145.07, new loss = -423.66(45.1 examples/sec; 0.709 sec/batch)
2018-10-09 15:05:59.934054: step 1620, G loss = 10405.27, new loss = 10.14(49.9 examples/sec; 0.642 sec/batch)
2018-10-09 15:06:06.601885: step 1630, G loss = 10449.45, new loss = -206.92(50.4 examples/sec; 0.634 sec/batch)
2018-10-09 15:06:13.197855: step 1640, G loss = 10262.70, new loss = -231.67(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:06:20.054460: step 1650, G loss = 10570.93, new loss = 1342.60(49.0 examples/sec; 0.653 sec/batch)
2018-10-09 15:06:26.612428: step 1660, G loss = 10738.37, new loss = 697.87(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:06:33.239411: step 1670, G loss = 9992.36, new loss = -887.89(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:06:39.839620: step 1680, G loss = 10777.05, new loss = -10.51(51.1 examples/sec; 0.627 sec/batch)
2018-10-09 15:06:46.524297: step 1690, G loss = 10779.61, new loss = -206.87(50.3 examples/sec; 0.636 sec/batch)
2018-10-09 15:06:53.128501: step 1700, G loss = 10423.67, new loss = 15.39(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:07:00.523007: step 1710, G loss = 10188.73, new loss = -417.06(45.2 examples/sec; 0.707 sec/batch)
2018-10-09 15:07:07.030240: step 1720, G loss = 10560.59, new loss = 40.54(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 15:07:13.495470: step 1730, G loss = 10293.98, new loss = -132.57(52.1 examples/sec; 0.614 sec/batch)
2018-10-09 15:07:20.111564: step 1740, G loss = 10513.00, new loss = 116.64(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:07:26.711763: step 1750, G loss = 10481.34, new loss = -124.86(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:07:33.250715: step 1760, G loss = 10669.65, new loss = -151.45(51.5 examples/sec; 0.621 sec/batch)
2018-10-09 15:07:39.868595: step 1770, G loss = 10799.18, new loss = -371.05(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:07:46.456251: step 1780, G loss = 10835.38, new loss = 243.91(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:07:53.006023: step 1790, G loss = 10246.71, new loss = -422.50(51.5 examples/sec; 0.621 sec/batch)
2018-10-09 15:07:59.582199: step 1800, G loss = 10864.32, new loss = 312.48(51.2 examples/sec; 0.626 sec/batch)
2018-10-09 15:08:06.979868: step 1810, G loss = 10182.91, new loss = -146.48(45.3 examples/sec; 0.706 sec/batch)
2018-10-09 15:08:13.742575: step 1820, G loss = 10196.13, new loss = -470.77(49.8 examples/sec; 0.642 sec/batch)
2018-10-09 15:08:20.338793: step 1830, G loss = 10678.34, new loss = -141.39(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:08:26.910953: step 1840, G loss = 10504.30, new loss = -309.18(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:08:33.461033: step 1850, G loss = 10287.25, new loss = -546.74(51.4 examples/sec; 0.622 sec/batch)
2018-10-09 15:08:40.005322: step 1860, G loss = 10668.23, new loss = 21.25(51.5 examples/sec; 0.621 sec/batch)
2018-10-09 15:08:46.555534: step 1870, G loss = 10820.59, new loss = 110.60(51.4 examples/sec; 0.622 sec/batch)
2018-10-09 15:08:53.143347: step 1880, G loss = 10627.38, new loss = -345.90(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:08:59.733629: step 1890, G loss = 10112.93, new loss = 158.84(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:09:06.370925: step 1900, G loss = 10131.93, new loss = -393.20(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:09:13.779413: step 1910, G loss = 10476.98, new loss = -662.03(45.2 examples/sec; 0.708 sec/batch)
2018-10-09 15:09:20.418612: step 1920, G loss = 10597.62, new loss = -145.24(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:09:27.039325: step 1930, G loss = 10119.09, new loss = 346.22(50.8 examples/sec; 0.629 sec/batch)
2018-10-09 15:09:33.605489: step 1940, G loss = 10767.34, new loss = 713.31(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:09:40.166825: step 1950, G loss = 10907.56, new loss = 185.47(51.3 examples/sec; 0.623 sec/batch)
2018-10-09 15:09:46.760789: step 1960, G loss = 10709.01, new loss = -94.59(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:09:53.267338: step 1970, G loss = 10665.26, new loss = -186.80(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 15:09:59.824616: step 1980, G loss = 10577.20, new loss = 554.48(51.4 examples/sec; 0.622 sec/batch)
2018-10-09 15:10:06.413007: step 1990, G loss = 10327.23, new loss = -412.94(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:10:12.986646: step 2000, G loss = 11162.27, new loss = -54.53(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:10:25.529553: step 2010, G loss = 10476.49, new loss = -234.67(26.2 examples/sec; 1.221 sec/batch)
2018-10-09 15:10:32.145300: step 2020, G loss = 10403.95, new loss = -773.27(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:10:38.752786: step 2030, G loss = 10877.17, new loss = 567.38(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:10:45.332215: step 2040, G loss = 10419.05, new loss = -691.93(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:10:51.875319: step 2050, G loss = 10568.15, new loss = 111.14(51.5 examples/sec; 0.621 sec/batch)
2018-10-09 15:10:58.369162: step 2060, G loss = 10678.96, new loss = 289.51(51.9 examples/sec; 0.617 sec/batch)
2018-10-09 15:11:04.948957: step 2070, G loss = 10492.41, new loss = 86.53(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:11:11.520949: step 2080, G loss = 10472.99, new loss = -33.46(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:11:18.094998: step 2090, G loss = 10055.81, new loss = -268.08(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:11:24.653410: step 2100, G loss = 10275.03, new loss = 253.11(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:11:31.997097: step 2110, G loss = 10388.89, new loss = 94.97(45.6 examples/sec; 0.701 sec/batch)
2018-10-09 15:11:38.557986: step 2120, G loss = 10125.52, new loss = -461.12(51.3 examples/sec; 0.623 sec/batch)
2018-10-09 15:11:45.132355: step 2130, G loss = 10347.55, new loss = -12.31(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:11:51.729115: step 2140, G loss = 10362.42, new loss = 85.70(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:11:58.262876: step 2150, G loss = 9732.02, new loss = -597.12(51.5 examples/sec; 0.621 sec/batch)
2018-10-09 15:12:04.842589: step 2160, G loss = 10229.91, new loss = -87.18(51.2 examples/sec; 0.626 sec/batch)
2018-10-09 15:12:10.613861: step 2170, G loss = 10353.75, new loss = -409.89(57.9 examples/sec; 0.553 sec/batch)
2018-10-09 15:12:16.259676: step 2180, G loss = 10171.72, new loss = -224.14(59.2 examples/sec; 0.541 sec/batch)
2018-10-09 15:12:21.930958: step 2190, G loss = 10664.33, new loss = 365.44(59.0 examples/sec; 0.543 sec/batch)
2018-10-09 15:12:27.575505: step 2200, G loss = 10240.45, new loss = -175.44(59.2 examples/sec; 0.540 sec/batch)
2018-10-09 15:12:34.094667: step 2210, G loss = 10611.28, new loss = 271.40(51.8 examples/sec; 0.618 sec/batch)
2018-10-09 15:12:39.761423: step 2220, G loss = 10260.45, new loss = -227.40(59.0 examples/sec; 0.542 sec/batch)
2018-10-09 15:12:45.417084: step 2230, G loss = 10566.43, new loss = 630.39(59.1 examples/sec; 0.541 sec/batch)
2018-10-09 15:12:51.026814: step 2240, G loss = 10573.76, new loss = 738.22(59.6 examples/sec; 0.537 sec/batch)
2018-10-09 15:12:56.668100: step 2250, G loss = 9974.92, new loss = -617.38(59.3 examples/sec; 0.539 sec/batch)
2018-10-09 15:13:02.320182: step 2260, G loss = 10313.76, new loss = -253.21(59.1 examples/sec; 0.541 sec/batch)
2018-10-09 15:13:08.013279: step 2270, G loss = 10459.38, new loss = 241.49(58.8 examples/sec; 0.545 sec/batch)
2018-10-09 15:13:13.700952: step 2280, G loss = 10009.87, new loss = -186.70(58.8 examples/sec; 0.544 sec/batch)
2018-10-09 15:13:19.380175: step 2290, G loss = 10119.62, new loss = 254.93(58.9 examples/sec; 0.543 sec/batch)
2018-10-09 15:13:25.045520: step 2300, G loss = 10614.11, new loss = 51.77(59.1 examples/sec; 0.541 sec/batch)
2018-10-09 15:13:31.370900: step 2310, G loss = 10065.82, new loss = -634.28(52.7 examples/sec; 0.607 sec/batch)
2018-10-09 15:13:36.989003: step 2320, G loss = 10989.40, new loss = 597.04(59.5 examples/sec; 0.538 sec/batch)
2018-10-09 15:13:42.615826: step 2330, G loss = 10354.84, new loss = -417.15(59.4 examples/sec; 0.539 sec/batch)
2018-10-09 15:13:48.247984: step 2340, G loss = 10608.20, new loss = -254.01(59.4 examples/sec; 0.539 sec/batch)
2018-10-09 15:13:53.911528: step 2350, G loss = 10193.21, new loss = -526.02(59.0 examples/sec; 0.543 sec/batch)
2018-10-09 15:13:59.617147: step 2360, G loss = 10586.19, new loss = -410.87(58.6 examples/sec; 0.546 sec/batch)
2018-10-09 15:14:05.217076: step 2370, G loss = 10382.73, new loss = 299.56(59.6 examples/sec; 0.537 sec/batch)
2018-10-09 15:14:10.871768: step 2380, G loss = 10564.68, new loss = -251.95(59.1 examples/sec; 0.541 sec/batch)
2018-10-09 15:14:16.559273: step 2390, G loss = 10563.42, new loss = 136.43(58.7 examples/sec; 0.545 sec/batch)
2018-10-09 15:14:22.187831: step 2400, G loss = 10555.54, new loss = -260.73(59.4 examples/sec; 0.539 sec/batch)
2018-10-09 15:14:28.711502: step 2410, G loss = 10313.11, new loss = -840.69(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:14:34.396187: step 2420, G loss = 10318.47, new loss = -550.41(58.8 examples/sec; 0.545 sec/batch)
2018-10-09 15:14:40.015021: step 2430, G loss = 10516.09, new loss = 81.86(59.5 examples/sec; 0.538 sec/batch)
2018-10-09 15:14:45.661305: step 2440, G loss = 10434.03, new loss = 173.75(59.2 examples/sec; 0.541 sec/batch)
2018-10-09 15:14:51.279349: step 2450, G loss = 10385.93, new loss = 60.55(59.5 examples/sec; 0.538 sec/batch)
2018-10-09 15:14:56.894307: step 2460, G loss = 10775.67, new loss = -186.04(59.4 examples/sec; 0.539 sec/batch)
2018-10-09 15:15:02.860238: step 2470, G loss = 10652.06, new loss = 190.36(56.7 examples/sec; 0.564 sec/batch)
2018-10-09 15:15:09.395093: step 2480, G loss = 10317.08, new loss = -231.53(51.5 examples/sec; 0.621 sec/batch)
2018-10-09 15:15:16.017047: step 2490, G loss = 10555.10, new loss = -95.98(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:15:22.584126: step 2500, G loss = 10804.22, new loss = 813.75(51.5 examples/sec; 0.622 sec/batch)
2018-10-09 15:15:29.942502: step 2510, G loss = 10411.04, new loss = 378.71(45.4 examples/sec; 0.705 sec/batch)
2018-10-09 15:15:36.477742: step 2520, G loss = 10197.38, new loss = 48.38(51.6 examples/sec; 0.621 sec/batch)
2018-10-09 15:15:43.019709: step 2530, G loss = 10186.88, new loss = -476.97(51.4 examples/sec; 0.622 sec/batch)
2018-10-09 15:15:49.579688: step 2540, G loss = 10525.53, new loss = 418.43(51.3 examples/sec; 0.623 sec/batch)
2018-10-09 15:15:56.146437: step 2550, G loss = 10656.84, new loss = 760.36(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:16:02.661871: step 2560, G loss = 10347.04, new loss = 785.06(51.9 examples/sec; 0.617 sec/batch)
2018-10-09 15:16:09.266361: step 2570, G loss = 10347.68, new loss = 40.09(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:16:15.854315: step 2580, G loss = 10685.56, new loss = 942.61(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:16:22.373199: step 2590, G loss = 10472.96, new loss = 465.41(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 15:16:28.910817: step 2600, G loss = 10472.64, new loss = -187.22(51.5 examples/sec; 0.622 sec/batch)
2018-10-09 15:16:36.186254: step 2610, G loss = 10433.90, new loss = 406.14(46.1 examples/sec; 0.695 sec/batch)
2018-10-09 15:16:42.713658: step 2620, G loss = 10379.24, new loss = -701.89(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 15:16:49.268903: step 2630, G loss = 10312.19, new loss = -435.47(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:16:55.899881: step 2640, G loss = 9698.93, new loss = -849.47(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:17:02.527698: step 2650, G loss = 10653.13, new loss = -120.16(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:17:09.096305: step 2660, G loss = 11123.17, new loss = 564.73(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:17:15.773693: step 2670, G loss = 10628.34, new loss = -514.49(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 15:17:22.369863: step 2680, G loss = 10139.53, new loss = 321.35(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:17:28.995858: step 2690, G loss = 10103.20, new loss = -333.31(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:17:35.536656: step 2700, G loss = 10061.78, new loss = -250.37(51.5 examples/sec; 0.621 sec/batch)
2018-10-09 15:17:42.931380: step 2710, G loss = 10663.20, new loss = 555.71(45.3 examples/sec; 0.706 sec/batch)
2018-10-09 15:17:49.535220: step 2720, G loss = 10489.08, new loss = -332.41(51.1 examples/sec; 0.627 sec/batch)
2018-10-09 15:17:56.145696: step 2730, G loss = 10687.53, new loss = 306.67(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:18:02.748803: step 2740, G loss = 10288.35, new loss = -161.02(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:18:09.370486: step 2750, G loss = 10258.84, new loss = -491.36(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:18:16.069393: step 2760, G loss = 10507.43, new loss = -33.74(50.3 examples/sec; 0.636 sec/batch)
2018-10-09 15:18:22.635471: step 2770, G loss = 10649.71, new loss = 160.21(51.4 examples/sec; 0.622 sec/batch)
2018-10-09 15:18:29.176420: step 2780, G loss = 10454.47, new loss = 305.43(51.5 examples/sec; 0.621 sec/batch)
2018-10-09 15:18:35.817613: step 2790, G loss = 10472.23, new loss = -84.29(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:18:42.285343: step 2800, G loss = 10870.30, new loss = 507.83(52.1 examples/sec; 0.615 sec/batch)
2018-10-09 15:18:49.536830: step 2810, G loss = 10739.86, new loss = 1006.69(46.3 examples/sec; 0.692 sec/batch)
2018-10-09 15:18:56.103846: step 2820, G loss = 11025.04, new loss = 95.42(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:19:02.686552: step 2830, G loss = 10684.87, new loss = 540.47(51.2 examples/sec; 0.624 sec/batch)
2018-10-09 15:19:09.310074: step 2840, G loss = 10428.97, new loss = -424.67(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:19:15.918433: step 2850, G loss = 10408.80, new loss = -370.33(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:19:22.554491: step 2860, G loss = 10279.54, new loss = -429.70(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:19:29.166655: step 2870, G loss = 10661.59, new loss = 83.06(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:19:35.851738: step 2880, G loss = 10288.48, new loss = -572.81(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 15:19:42.471017: step 2890, G loss = 10653.23, new loss = 523.03(50.8 examples/sec; 0.629 sec/batch)
2018-10-09 15:19:49.173214: step 2900, G loss = 10465.26, new loss = -436.48(50.3 examples/sec; 0.636 sec/batch)
2018-10-09 15:19:56.614584: step 2910, G loss = 10935.56, new loss = 816.87(45.0 examples/sec; 0.710 sec/batch)
2018-10-09 15:20:03.321093: step 2920, G loss = 10411.99, new loss = -501.92(50.3 examples/sec; 0.636 sec/batch)
2018-10-09 15:20:09.889175: step 2930, G loss = 10634.35, new loss = 578.20(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:20:16.489743: step 2940, G loss = 10515.75, new loss = -11.49(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:20:23.081110: step 2950, G loss = 10024.68, new loss = -169.12(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:20:29.706147: step 2960, G loss = 10347.85, new loss = 294.61(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:20:36.190228: step 2970, G loss = 10383.48, new loss = -563.53(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:20:41.923064: step 2980, G loss = 10948.07, new loss = 2252.31(58.3 examples/sec; 0.549 sec/batch)
2018-10-09 15:20:47.598268: step 2990, G loss = 10455.53, new loss = 165.07(59.0 examples/sec; 0.542 sec/batch)
2018-10-09 15:20:53.273368: step 3000, G loss = 10628.43, new loss = 3.06(58.9 examples/sec; 0.543 sec/batch)
2018-10-09 15:21:04.821605: step 3010, G loss = 11014.60, new loss = 1126.19(28.3 examples/sec; 1.131 sec/batch)
2018-10-09 15:21:10.489510: step 3020, G loss = 10318.01, new loss = -341.58(59.0 examples/sec; 0.542 sec/batch)
2018-10-09 15:21:16.263544: step 3030, G loss = 10542.96, new loss = -427.21(57.9 examples/sec; 0.553 sec/batch)
2018-10-09 15:21:21.930495: step 3040, G loss = 10584.54, new loss = -237.38(59.1 examples/sec; 0.542 sec/batch)
2018-10-09 15:21:27.606042: step 3050, G loss = 10827.31, new loss = 1407.44(59.0 examples/sec; 0.543 sec/batch)
2018-10-09 15:21:33.281344: step 3060, G loss = 10290.24, new loss = -277.03(58.9 examples/sec; 0.543 sec/batch)
2018-10-09 15:21:38.925206: step 3070, G loss = 10335.49, new loss = -313.88(59.3 examples/sec; 0.540 sec/batch)
2018-10-09 15:21:44.574171: step 3080, G loss = 10455.95, new loss = -226.79(59.2 examples/sec; 0.541 sec/batch)
2018-10-09 15:21:50.252504: step 3090, G loss = 10497.30, new loss = 123.05(58.9 examples/sec; 0.544 sec/batch)
2018-10-09 15:21:55.893751: step 3100, G loss = 10259.25, new loss = -466.47(59.3 examples/sec; 0.540 sec/batch)
2018-10-09 15:22:02.293283: step 3110, G loss = 10198.97, new loss = -443.59(52.0 examples/sec; 0.616 sec/batch)
2018-10-09 15:22:07.953795: step 3120, G loss = 9996.65, new loss = -500.43(59.0 examples/sec; 0.542 sec/batch)
2018-10-09 15:22:13.616564: step 3130, G loss = 10410.42, new loss = -611.78(58.9 examples/sec; 0.543 sec/batch)
2018-10-09 15:22:19.297396: step 3140, G loss = 10506.33, new loss = -261.12(58.9 examples/sec; 0.544 sec/batch)
2018-10-09 15:22:24.961184: step 3150, G loss = 10645.97, new loss = -317.51(59.1 examples/sec; 0.542 sec/batch)
2018-10-09 15:22:30.656434: step 3160, G loss = 10549.71, new loss = -240.99(58.7 examples/sec; 0.545 sec/batch)
2018-10-09 15:22:36.322222: step 3170, G loss = 10197.29, new loss = -60.54(59.0 examples/sec; 0.543 sec/batch)
2018-10-09 15:22:41.943599: step 3180, G loss = 10396.11, new loss = 395.29(59.5 examples/sec; 0.538 sec/batch)
2018-10-09 15:22:47.604648: step 3190, G loss = 10545.27, new loss = -34.82(59.1 examples/sec; 0.542 sec/batch)
2018-10-09 15:22:53.333717: step 3200, G loss = 10387.30, new loss = 283.31(58.3 examples/sec; 0.549 sec/batch)
2018-10-09 15:22:59.766328: step 3210, G loss = 10512.65, new loss = -667.71(51.8 examples/sec; 0.618 sec/batch)
2018-10-09 15:23:05.372608: step 3220, G loss = 10460.83, new loss = -382.36(59.7 examples/sec; 0.536 sec/batch)
2018-10-09 15:23:11.076910: step 3230, G loss = 10256.53, new loss = -267.20(58.5 examples/sec; 0.547 sec/batch)
2018-10-09 15:23:16.776280: step 3240, G loss = 10502.91, new loss = -405.67(58.7 examples/sec; 0.545 sec/batch)
2018-10-09 15:23:22.459614: step 3250, G loss = 10594.85, new loss = 391.77(58.8 examples/sec; 0.544 sec/batch)
2018-10-09 15:23:28.447565: step 3260, G loss = 10540.90, new loss = 102.33(56.6 examples/sec; 0.565 sec/batch)
2018-10-09 15:23:35.013341: step 3270, G loss = 10727.46, new loss = 464.59(51.5 examples/sec; 0.622 sec/batch)
2018-10-09 15:23:41.585038: step 3280, G loss = 10703.60, new loss = 244.25(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:23:48.127465: step 3290, G loss = 10386.29, new loss = -557.49(51.5 examples/sec; 0.622 sec/batch)
2018-10-09 15:23:54.698785: step 3300, G loss = 10843.24, new loss = 228.22(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:24:02.054126: step 3310, G loss = 10736.62, new loss = 573.10(45.5 examples/sec; 0.703 sec/batch)
2018-10-09 15:24:08.608107: step 3320, G loss = 10345.94, new loss = -725.29(51.3 examples/sec; 0.623 sec/batch)
2018-10-09 15:24:15.178408: step 3330, G loss = 10701.71, new loss = 366.48(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:24:21.732765: step 3340, G loss = 10574.95, new loss = -82.47(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:24:28.215655: step 3350, G loss = 10662.61, new loss = -59.40(51.9 examples/sec; 0.617 sec/batch)
2018-10-09 15:24:34.826583: step 3360, G loss = 10672.98, new loss = 787.67(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:24:41.385176: step 3370, G loss = 10444.93, new loss = -168.06(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:24:48.042236: step 3380, G loss = 10394.09, new loss = -484.06(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:24:54.654608: step 3390, G loss = 10711.03, new loss = 679.38(50.8 examples/sec; 0.629 sec/batch)
2018-10-09 15:25:01.241401: step 3400, G loss = 10165.01, new loss = -421.61(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:25:08.643917: step 3410, G loss = 10811.51, new loss = -116.48(45.3 examples/sec; 0.707 sec/batch)
2018-10-09 15:25:15.280564: step 3420, G loss = 10423.88, new loss = -316.35(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:25:21.854169: step 3430, G loss = 10089.25, new loss = -379.40(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:25:28.465740: step 3440, G loss = 10557.88, new loss = 544.23(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:25:35.104249: step 3450, G loss = 10544.73, new loss = 170.59(50.7 examples/sec; 0.632 sec/batch)
2018-10-09 15:25:41.749083: step 3460, G loss = 10794.22, new loss = 192.67(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:25:48.406436: step 3470, G loss = 10032.74, new loss = -464.21(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:25:55.034880: step 3480, G loss = 9770.48, new loss = -566.38(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:26:01.667607: step 3490, G loss = 10596.82, new loss = 673.55(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:26:08.224310: step 3500, G loss = 10527.19, new loss = -390.51(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:26:15.568806: step 3510, G loss = 10557.83, new loss = -36.67(45.5 examples/sec; 0.703 sec/batch)
2018-10-09 15:26:22.167807: step 3520, G loss = 10174.42, new loss = -27.75(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:26:28.839444: step 3530, G loss = 10341.91, new loss = -362.68(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:26:35.457576: step 3540, G loss = 10225.38, new loss = 18.37(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:26:42.097895: step 3550, G loss = 10062.46, new loss = -419.13(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:26:48.687947: step 3560, G loss = 10249.38, new loss = -147.27(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:26:55.220496: step 3570, G loss = 10872.54, new loss = -281.10(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 15:27:01.811767: step 3580, G loss = 10776.69, new loss = -542.22(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:27:08.412058: step 3590, G loss = 10311.14, new loss = -332.63(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:27:14.999654: step 3600, G loss = 10559.14, new loss = -1.78(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:27:22.359323: step 3610, G loss = 10259.42, new loss = -498.58(45.6 examples/sec; 0.702 sec/batch)
2018-10-09 15:27:29.001934: step 3620, G loss = 10266.94, new loss = -317.74(50.8 examples/sec; 0.629 sec/batch)
2018-10-09 15:27:35.640758: step 3630, G loss = 10333.36, new loss = 243.93(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:27:42.263618: step 3640, G loss = 10332.05, new loss = -596.42(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:27:48.934827: step 3650, G loss = 10515.55, new loss = 337.48(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 15:27:55.535817: step 3660, G loss = 10697.21, new loss = 644.67(51.1 examples/sec; 0.627 sec/batch)
2018-10-09 15:28:02.150736: step 3670, G loss = 10314.91, new loss = 74.35(50.8 examples/sec; 0.629 sec/batch)
2018-10-09 15:28:08.763937: step 3680, G loss = 10471.07, new loss = -125.64(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:28:15.352946: step 3690, G loss = 10194.28, new loss = -464.78(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:28:22.000272: step 3700, G loss = 10163.94, new loss = -405.74(50.7 examples/sec; 0.632 sec/batch)
2018-10-09 15:28:29.226810: step 3710, G loss = 10794.28, new loss = -140.27(46.3 examples/sec; 0.691 sec/batch)
2018-10-09 15:28:35.883553: step 3720, G loss = 10394.22, new loss = -128.95(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:28:42.466384: step 3730, G loss = 10482.79, new loss = -299.92(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:28:49.034781: step 3740, G loss = 10469.79, new loss = 754.50(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:28:55.646557: step 3750, G loss = 10206.42, new loss = -581.79(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:29:02.307151: step 3760, G loss = 10355.60, new loss = -205.13(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:29:08.941857: step 3770, G loss = 10374.38, new loss = -466.63(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:29:15.590782: step 3780, G loss = 10458.65, new loss = -381.29(50.7 examples/sec; 0.632 sec/batch)
2018-10-09 15:29:22.278651: step 3790, G loss = 10389.39, new loss = 240.89(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 15:29:28.904458: step 3800, G loss = 10279.04, new loss = -149.49(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:29:36.227369: step 3810, G loss = 10657.45, new loss = 312.28(45.7 examples/sec; 0.700 sec/batch)
2018-10-09 15:29:42.867628: step 3820, G loss = 10351.29, new loss = -364.88(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:29:49.456681: step 3830, G loss = 10236.51, new loss = 69.78(51.1 examples/sec; 0.627 sec/batch)
2018-10-09 15:29:56.229625: step 3840, G loss = 10469.17, new loss = 200.75(49.7 examples/sec; 0.644 sec/batch)
2018-10-09 15:30:02.981395: step 3850, G loss = 10632.17, new loss = 937.91(49.8 examples/sec; 0.642 sec/batch)
2018-10-09 15:30:09.539675: step 3860, G loss = 10552.96, new loss = -264.08(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:30:16.157387: step 3870, G loss = 10328.40, new loss = -354.88(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:30:22.825053: step 3880, G loss = 10388.21, new loss = 262.61(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:30:29.523726: step 3890, G loss = 10411.71, new loss = -420.48(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:30:36.133604: step 3900, G loss = 10514.18, new loss = 182.73(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:30:43.638531: step 3910, G loss = 10490.76, new loss = 68.57(44.7 examples/sec; 0.717 sec/batch)
2018-10-09 15:30:50.295660: step 3920, G loss = 10232.10, new loss = -297.79(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:30:56.871725: step 3930, G loss = 10357.19, new loss = -391.86(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:31:03.352766: step 3940, G loss = 10299.07, new loss = 118.73(51.9 examples/sec; 0.617 sec/batch)
2018-10-09 15:31:09.945281: step 3950, G loss = 10437.05, new loss = 8.43(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:31:16.557486: step 3960, G loss = 10681.06, new loss = 392.89(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:31:23.213471: step 3970, G loss = 10675.35, new loss = -173.59(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:31:29.842525: step 3980, G loss = 10502.47, new loss = -151.55(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:31:36.358490: step 3990, G loss = 10621.76, new loss = 298.62(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 15:31:42.942769: step 4000, G loss = 10771.25, new loss = 54.61(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:31:56.833258: step 4010, G loss = 10248.84, new loss = -339.05(23.6 examples/sec; 1.355 sec/batch)
2018-10-09 15:32:03.420306: step 4020, G loss = 10243.25, new loss = 121.52(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:32:10.002811: step 4030, G loss = 10852.14, new loss = -409.38(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:32:16.594138: step 4040, G loss = 10299.57, new loss = -490.85(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:32:23.143445: step 4050, G loss = 10258.81, new loss = -305.00(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:32:29.746248: step 4060, G loss = 10468.12, new loss = -432.75(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:32:36.392147: step 4070, G loss = 9943.18, new loss = -154.67(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:32:42.298975: step 4080, G loss = 10174.00, new loss = -408.84(56.5 examples/sec; 0.566 sec/batch)
2018-10-09 15:32:47.972417: step 4090, G loss = 10452.47, new loss = 152.14(59.0 examples/sec; 0.543 sec/batch)
2018-10-09 15:32:53.638245: step 4100, G loss = 10311.62, new loss = -157.94(59.0 examples/sec; 0.542 sec/batch)
2018-10-09 15:33:00.457041: step 4110, G loss = 10726.71, new loss = 537.34(48.6 examples/sec; 0.658 sec/batch)
2018-10-09 15:33:06.115713: step 4120, G loss = 10441.71, new loss = 189.11(59.1 examples/sec; 0.542 sec/batch)
2018-10-09 15:33:11.767537: step 4130, G loss = 10307.00, new loss = 7.55(59.2 examples/sec; 0.541 sec/batch)
2018-10-09 15:33:17.427584: step 4140, G loss = 10330.31, new loss = -324.77(59.2 examples/sec; 0.541 sec/batch)
2018-10-09 15:33:23.088500: step 4150, G loss = 10574.46, new loss = 538.25(59.2 examples/sec; 0.541 sec/batch)
2018-10-09 15:33:28.742945: step 4160, G loss = 9884.73, new loss = 980.62(59.1 examples/sec; 0.541 sec/batch)
2018-10-09 15:33:34.524737: step 4170, G loss = 10325.18, new loss = -682.11(57.8 examples/sec; 0.554 sec/batch)
2018-10-09 15:33:40.175144: step 4180, G loss = 10266.56, new loss = -197.13(59.2 examples/sec; 0.541 sec/batch)
2018-10-09 15:33:45.812071: step 4190, G loss = 10418.96, new loss = -121.10(59.3 examples/sec; 0.539 sec/batch)
2018-10-09 15:33:51.482832: step 4200, G loss = 10080.00, new loss = -129.13(59.0 examples/sec; 0.542 sec/batch)
2018-10-09 15:33:57.867441: step 4210, G loss = 10417.60, new loss = 368.77(52.1 examples/sec; 0.615 sec/batch)
2018-10-09 15:34:03.506447: step 4220, G loss = 10086.96, new loss = -852.60(59.2 examples/sec; 0.540 sec/batch)
2018-10-09 15:34:09.189661: step 4230, G loss = 10389.96, new loss = -586.60(58.8 examples/sec; 0.544 sec/batch)
2018-10-09 15:34:14.866479: step 4240, G loss = 10529.56, new loss = 306.32(58.9 examples/sec; 0.544 sec/batch)
2018-10-09 15:34:20.573400: step 4250, G loss = 10700.41, new loss = 127.72(58.6 examples/sec; 0.546 sec/batch)
2018-10-09 15:34:26.237359: step 4260, G loss = 10610.58, new loss = 1417.16(59.0 examples/sec; 0.542 sec/batch)
2018-10-09 15:34:31.904486: step 4270, G loss = 10651.26, new loss = -213.45(59.0 examples/sec; 0.543 sec/batch)
2018-10-09 15:34:37.538241: step 4280, G loss = 10649.03, new loss = 165.10(59.4 examples/sec; 0.539 sec/batch)
2018-10-09 15:34:43.208744: step 4290, G loss = 10508.85, new loss = -308.23(58.9 examples/sec; 0.543 sec/batch)
2018-10-09 15:34:48.850137: step 4300, G loss = 9962.06, new loss = -82.29(59.3 examples/sec; 0.540 sec/batch)
2018-10-09 15:34:55.214415: step 4310, G loss = 10053.93, new loss = -574.29(52.3 examples/sec; 0.612 sec/batch)
2018-10-09 15:35:00.833368: step 4320, G loss = 10376.26, new loss = 285.79(59.6 examples/sec; 0.537 sec/batch)
2018-10-09 15:35:06.512291: step 4330, G loss = 10537.58, new loss = 505.45(58.8 examples/sec; 0.544 sec/batch)
2018-10-09 15:35:12.128998: step 4340, G loss = 10477.60, new loss = 181.24(59.5 examples/sec; 0.538 sec/batch)
2018-10-09 15:35:18.767386: step 4350, G loss = 10228.87, new loss = -264.63(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:35:25.410667: step 4360, G loss = 11064.66, new loss = 1235.21(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:35:32.054875: step 4370, G loss = 10235.07, new loss = -343.76(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:35:38.656230: step 4380, G loss = 10442.70, new loss = 13.01(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:35:45.309135: step 4390, G loss = 10441.58, new loss = 162.98(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:35:51.954324: step 4400, G loss = 10547.72, new loss = -215.21(50.7 examples/sec; 0.632 sec/batch)
2018-10-09 15:35:59.255973: step 4410, G loss = 10294.88, new loss = 83.39(46.0 examples/sec; 0.695 sec/batch)
2018-10-09 15:36:05.842112: step 4420, G loss = 10225.04, new loss = 96.36(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:36:12.503338: step 4430, G loss = 10020.30, new loss = -686.70(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 15:36:19.067884: step 4440, G loss = 10344.54, new loss = 409.05(51.4 examples/sec; 0.622 sec/batch)
2018-10-09 15:36:25.740035: step 4450, G loss = 10399.02, new loss = -247.82(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:36:32.394604: step 4460, G loss = 10981.88, new loss = 1094.29(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:36:38.994788: step 4470, G loss = 10249.62, new loss = -399.72(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:36:45.545461: step 4480, G loss = 10891.45, new loss = -129.83(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:36:52.110432: step 4490, G loss = 10444.00, new loss = -203.56(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:36:58.753885: step 4500, G loss = 11178.01, new loss = 1858.45(50.7 examples/sec; 0.632 sec/batch)
2018-10-09 15:37:06.157688: step 4510, G loss = 10505.27, new loss = -529.88(45.2 examples/sec; 0.707 sec/batch)
2018-10-09 15:37:12.680140: step 4520, G loss = 10724.90, new loss = 104.88(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 15:37:19.256123: step 4530, G loss = 10358.21, new loss = -297.39(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:37:25.832179: step 4540, G loss = 10578.21, new loss = -389.93(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:37:32.400828: step 4550, G loss = 10195.44, new loss = -429.12(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:37:38.955125: step 4560, G loss = 10391.00, new loss = 527.30(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:37:45.450011: step 4570, G loss = 10016.90, new loss = -99.89(51.8 examples/sec; 0.618 sec/batch)
2018-10-09 15:37:52.054136: step 4580, G loss = 10333.58, new loss = -326.43(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:37:58.589068: step 4590, G loss = 10216.73, new loss = -312.47(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 15:38:05.153885: step 4600, G loss = 10256.09, new loss = 404.22(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:38:12.605979: step 4610, G loss = 10814.66, new loss = 286.44(45.0 examples/sec; 0.711 sec/batch)
2018-10-09 15:38:19.183272: step 4620, G loss = 10490.14, new loss = 523.46(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:38:25.856821: step 4630, G loss = 10106.87, new loss = -69.55(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 15:38:32.490872: step 4640, G loss = 10479.05, new loss = -662.64(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:38:39.051681: step 4650, G loss = 11087.89, new loss = 463.06(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:38:45.650948: step 4660, G loss = 10799.88, new loss = 92.99(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:38:52.277329: step 4670, G loss = 10243.98, new loss = -583.38(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:38:58.826880: step 4680, G loss = 10511.40, new loss = -220.01(51.4 examples/sec; 0.622 sec/batch)
2018-10-09 15:39:05.525253: step 4690, G loss = 10262.86, new loss = -427.15(50.3 examples/sec; 0.637 sec/batch)
2018-10-09 15:39:12.207434: step 4700, G loss = 10415.16, new loss = -364.16(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 15:39:19.610360: step 4710, G loss = 10400.57, new loss = -668.80(45.2 examples/sec; 0.709 sec/batch)
2018-10-09 15:39:26.266897: step 4720, G loss = 10711.38, new loss = 1238.35(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:39:32.934289: step 4730, G loss = 9899.77, new loss = -369.46(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 15:39:39.521498: step 4740, G loss = 10174.38, new loss = -129.53(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:39:46.149722: step 4750, G loss = 10729.77, new loss = -263.10(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:39:52.755269: step 4760, G loss = 10255.95, new loss = -258.70(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:39:59.355421: step 4770, G loss = 10551.77, new loss = 202.72(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:40:05.866826: step 4780, G loss = 10194.88, new loss = -178.02(51.7 examples/sec; 0.618 sec/batch)
2018-10-09 15:40:12.430182: step 4790, G loss = 10422.08, new loss = -235.43(51.3 examples/sec; 0.623 sec/batch)
2018-10-09 15:40:19.018088: step 4800, G loss = 10290.96, new loss = -199.06(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:40:26.405116: step 4810, G loss = 10316.47, new loss = 204.12(45.3 examples/sec; 0.707 sec/batch)
2018-10-09 15:40:33.023996: step 4820, G loss = 10225.34, new loss = -212.76(50.8 examples/sec; 0.629 sec/batch)
2018-10-09 15:40:39.626317: step 4830, G loss = 10395.74, new loss = -100.31(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:40:46.145189: step 4840, G loss = 10422.48, new loss = -308.81(51.7 examples/sec; 0.619 sec/batch)
2018-10-09 15:40:52.689251: step 4850, G loss = 10556.43, new loss = 27.63(51.4 examples/sec; 0.622 sec/batch)
2018-10-09 15:40:59.272511: step 4860, G loss = 10137.08, new loss = -179.50(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:41:05.840336: step 4870, G loss = 10636.22, new loss = -302.14(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:41:12.380517: step 4880, G loss = 10191.30, new loss = -642.89(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 15:41:18.928111: step 4890, G loss = 10159.30, new loss = -348.64(51.4 examples/sec; 0.622 sec/batch)
2018-10-09 15:41:25.520101: step 4900, G loss = 10312.62, new loss = -536.78(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:41:32.896720: step 4910, G loss = 10417.52, new loss = 33.41(45.4 examples/sec; 0.705 sec/batch)
2018-10-09 15:41:39.393192: step 4920, G loss = 10793.99, new loss = 88.14(51.8 examples/sec; 0.617 sec/batch)
2018-10-09 15:41:46.022316: step 4930, G loss = 10655.72, new loss = 268.54(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:41:52.549390: step 4940, G loss = 10548.64, new loss = -256.63(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 15:41:59.191784: step 4950, G loss = 10476.74, new loss = -100.25(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:42:05.676969: step 4960, G loss = 9966.05, new loss = -528.76(51.8 examples/sec; 0.618 sec/batch)
2018-10-09 15:42:12.260467: step 4970, G loss = 10365.99, new loss = 77.84(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:42:18.807115: step 4980, G loss = 10138.07, new loss = -525.35(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:42:25.348907: step 4990, G loss = 10181.91, new loss = 2.90(51.5 examples/sec; 0.622 sec/batch)
2018-10-09 15:42:31.949690: step 5000, G loss = 10327.96, new loss = 232.26(51.1 examples/sec; 0.627 sec/batch)
2018-10-09 15:42:44.962496: step 5010, G loss = 10545.01, new loss = 82.36(25.2 examples/sec; 1.268 sec/batch)
2018-10-09 15:42:51.536196: step 5020, G loss = 10119.12, new loss = -476.35(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:42:58.072129: step 5030, G loss = 10588.71, new loss = 371.30(51.5 examples/sec; 0.622 sec/batch)
2018-10-09 15:43:04.732124: step 5040, G loss = 11034.27, new loss = 640.26(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 15:43:11.299808: step 5050, G loss = 10270.50, new loss = -239.48(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:43:17.932023: step 5060, G loss = 10528.98, new loss = -425.69(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:43:24.475492: step 5070, G loss = 10307.72, new loss = 381.30(51.6 examples/sec; 0.620 sec/batch)
2018-10-09 15:43:31.019075: step 5080, G loss = 10183.98, new loss = -443.56(51.4 examples/sec; 0.622 sec/batch)
2018-10-09 15:43:37.632212: step 5090, G loss = 10372.33, new loss = -48.27(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:43:44.232164: step 5100, G loss = 10403.29, new loss = 33.08(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:43:51.513236: step 5110, G loss = 10359.69, new loss = -147.14(46.0 examples/sec; 0.696 sec/batch)
2018-10-09 15:43:58.112496: step 5120, G loss = 10635.79, new loss = -218.92(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:44:04.775632: step 5130, G loss = 10209.11, new loss = 58.99(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 15:44:11.453943: step 5140, G loss = 10291.10, new loss = -158.18(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 15:44:18.077694: step 5150, G loss = 9818.43, new loss = -812.18(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:44:24.593789: step 5160, G loss = 10545.87, new loss = 438.42(51.7 examples/sec; 0.619 sec/batch)
2018-10-09 15:44:31.140807: step 5170, G loss = 10365.53, new loss = 51.17(51.5 examples/sec; 0.622 sec/batch)
2018-10-09 15:44:37.723958: step 5180, G loss = 10353.02, new loss = -476.58(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:44:44.313858: step 5190, G loss = 10421.14, new loss = -228.76(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:44:50.880816: step 5200, G loss = 10019.81, new loss = -168.42(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:44:58.318399: step 5210, G loss = 10559.29, new loss = 666.65(45.0 examples/sec; 0.712 sec/batch)
2018-10-09 15:45:04.934900: step 5220, G loss = 10376.61, new loss = -157.17(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:45:11.578141: step 5230, G loss = 10588.91, new loss = 115.78(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:45:18.234919: step 5240, G loss = 10246.90, new loss = -765.83(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:45:24.847150: step 5250, G loss = 10974.02, new loss = 0.98(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:45:31.458477: step 5260, G loss = 10457.74, new loss = -411.29(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:45:38.037544: step 5270, G loss = 10803.84, new loss = -350.40(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:45:44.498456: step 5280, G loss = 10435.91, new loss = 360.16(52.1 examples/sec; 0.614 sec/batch)
2018-10-09 15:45:51.070664: step 5290, G loss = 10424.53, new loss = 1203.11(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:45:57.671668: step 5300, G loss = 10608.71, new loss = 277.03(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:46:05.144362: step 5310, G loss = 10081.97, new loss = -167.79(44.7 examples/sec; 0.715 sec/batch)
2018-10-09 15:46:11.650889: step 5320, G loss = 10229.45, new loss = 327.77(51.7 examples/sec; 0.619 sec/batch)
2018-10-09 15:46:18.278837: step 5330, G loss = 10715.49, new loss = 258.15(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:46:24.869332: step 5340, G loss = 10298.39, new loss = -352.68(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:46:31.418165: step 5350, G loss = 9796.51, new loss = -593.52(51.3 examples/sec; 0.623 sec/batch)
2018-10-09 15:46:38.034229: step 5360, G loss = 10247.25, new loss = -471.01(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:46:44.615761: step 5370, G loss = 10854.04, new loss = 665.21(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:46:51.140735: step 5380, G loss = 10568.59, new loss = 745.68(51.5 examples/sec; 0.621 sec/batch)
2018-10-09 15:46:57.707938: step 5390, G loss = 9917.03, new loss = -284.63(51.5 examples/sec; 0.622 sec/batch)
2018-10-09 15:47:04.216827: step 5400, G loss = 10792.23, new loss = 428.74(51.7 examples/sec; 0.619 sec/batch)
2018-10-09 15:47:12.745043: step 5410, G loss = 10637.65, new loss = 318.89(39.0 examples/sec; 0.820 sec/batch)
2018-10-09 15:47:19.357169: step 5420, G loss = 10169.65, new loss = -402.96(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:47:25.891581: step 5430, G loss = 10490.49, new loss = -245.21(51.6 examples/sec; 0.621 sec/batch)
2018-10-09 15:47:32.479825: step 5440, G loss = 10383.81, new loss = -89.59(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:47:38.989134: step 5450, G loss = 10346.81, new loss = 346.49(51.7 examples/sec; 0.619 sec/batch)
2018-10-09 15:47:45.578286: step 5460, G loss = 9950.14, new loss = -405.58(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:47:52.204043: step 5470, G loss = 10787.74, new loss = 1184.02(50.8 examples/sec; 0.629 sec/batch)
2018-10-09 15:47:58.751283: step 5480, G loss = 10652.68, new loss = -220.37(51.5 examples/sec; 0.622 sec/batch)
2018-10-09 15:48:05.371260: step 5490, G loss = 10790.86, new loss = 363.30(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:48:12.004386: step 5500, G loss = 10230.64, new loss = 98.59(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:48:22.252001: step 5510, G loss = 10786.46, new loss = 512.95(32.3 examples/sec; 0.992 sec/batch)
2018-10-09 15:48:28.890582: step 5520, G loss = 10230.80, new loss = -419.91(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:48:35.481824: step 5530, G loss = 10191.65, new loss = 353.42(51.1 examples/sec; 0.627 sec/batch)
2018-10-09 15:48:42.144237: step 5540, G loss = 10331.26, new loss = 39.39(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 15:48:48.830665: step 5550, G loss = 10659.96, new loss = -30.95(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 15:48:55.471215: step 5560, G loss = 10812.97, new loss = 618.99(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:49:02.010588: step 5570, G loss = 11154.88, new loss = 730.33(51.6 examples/sec; 0.621 sec/batch)
2018-10-09 15:49:08.592793: step 5580, G loss = 10501.21, new loss = 309.95(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:49:15.110017: step 5590, G loss = 10633.20, new loss = 730.24(51.7 examples/sec; 0.619 sec/batch)
2018-10-09 15:49:21.677562: step 5600, G loss = 10365.08, new loss = 312.81(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:49:29.117642: step 5610, G loss = 10171.55, new loss = -497.80(45.0 examples/sec; 0.711 sec/batch)
2018-10-09 15:49:35.724217: step 5620, G loss = 10458.81, new loss = -120.68(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:49:42.316621: step 5630, G loss = 10504.48, new loss = 115.25(51.1 examples/sec; 0.627 sec/batch)
2018-10-09 15:49:48.968990: step 5640, G loss = 10012.49, new loss = -618.83(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 15:49:55.560163: step 5650, G loss = 10875.63, new loss = -186.06(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:50:02.176330: step 5660, G loss = 10144.45, new loss = 249.65(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:50:08.762325: step 5670, G loss = 10538.18, new loss = 69.49(51.2 examples/sec; 0.626 sec/batch)
2018-10-09 15:50:15.403765: step 5680, G loss = 10542.59, new loss = 476.34(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:50:22.037613: step 5690, G loss = 10276.83, new loss = -386.83(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:50:28.695961: step 5700, G loss = 10153.35, new loss = -793.56(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:50:36.144439: step 5710, G loss = 10108.29, new loss = -666.76(44.9 examples/sec; 0.713 sec/batch)
2018-10-09 15:50:42.669944: step 5720, G loss = 10491.87, new loss = 338.28(51.7 examples/sec; 0.619 sec/batch)
2018-10-09 15:50:49.254521: step 5730, G loss = 10305.81, new loss = -246.28(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:50:55.847572: step 5740, G loss = 10665.73, new loss = 1122.88(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:51:02.414954: step 5750, G loss = 10255.57, new loss = 105.74(51.3 examples/sec; 0.623 sec/batch)
2018-10-09 15:51:08.888490: step 5760, G loss = 10174.34, new loss = -360.47(52.1 examples/sec; 0.615 sec/batch)
2018-10-09 15:51:15.464115: step 5770, G loss = 10947.61, new loss = 286.02(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:51:22.079232: step 5780, G loss = 10422.16, new loss = -98.20(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:51:28.686927: step 5790, G loss = 10666.31, new loss = 8.36(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:51:35.262831: step 5800, G loss = 10554.85, new loss = 404.00(51.2 examples/sec; 0.624 sec/batch)
2018-10-09 15:51:42.719857: step 5810, G loss = 10542.45, new loss = 315.85(44.9 examples/sec; 0.713 sec/batch)
2018-10-09 15:51:49.400197: step 5820, G loss = 10153.49, new loss = 256.92(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 15:51:56.000445: step 5830, G loss = 10377.31, new loss = -551.53(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:52:02.582408: step 5840, G loss = 10588.10, new loss = -140.48(51.1 examples/sec; 0.627 sec/batch)
2018-10-09 15:52:09.228236: step 5850, G loss = 10307.40, new loss = -47.65(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:52:15.897565: step 5860, G loss = 10451.37, new loss = -295.71(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:52:22.507146: step 5870, G loss = 10902.88, new loss = -152.11(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:52:29.119141: step 5880, G loss = 10270.35, new loss = 121.83(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:52:35.742345: step 5890, G loss = 9848.98, new loss = -579.61(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:52:42.374358: step 5900, G loss = 10583.70, new loss = -29.77(50.8 examples/sec; 0.629 sec/batch)
2018-10-09 15:52:49.726017: step 5910, G loss = 10475.68, new loss = 149.44(45.5 examples/sec; 0.703 sec/batch)
2018-10-09 15:52:56.326370: step 5920, G loss = 10260.54, new loss = 227.79(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:53:03.022127: step 5930, G loss = 10360.29, new loss = -142.99(50.2 examples/sec; 0.637 sec/batch)
2018-10-09 15:53:09.736521: step 5940, G loss = 10453.31, new loss = 259.68(50.1 examples/sec; 0.639 sec/batch)
2018-10-09 15:53:16.441057: step 5950, G loss = 10554.54, new loss = 51.26(50.2 examples/sec; 0.637 sec/batch)
2018-10-09 15:53:22.993474: step 5960, G loss = 10461.43, new loss = -269.90(51.5 examples/sec; 0.622 sec/batch)
2018-10-09 15:53:29.700094: step 5970, G loss = 10171.82, new loss = -345.14(50.2 examples/sec; 0.637 sec/batch)
2018-10-09 15:53:36.355342: step 5980, G loss = 10313.81, new loss = -521.62(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:53:42.952822: step 5990, G loss = 10026.59, new loss = -748.78(51.1 examples/sec; 0.627 sec/batch)
2018-10-09 15:53:49.513272: step 6000, G loss = 10481.77, new loss = -630.42(51.5 examples/sec; 0.622 sec/batch)
2018-10-09 15:54:02.111380: step 6010, G loss = 10082.33, new loss = -667.96(26.1 examples/sec; 1.227 sec/batch)
2018-10-09 15:54:08.680020: step 6020, G loss = 10165.29, new loss = -494.96(51.2 examples/sec; 0.624 sec/batch)
2018-10-09 15:54:15.351411: step 6030, G loss = 10297.73, new loss = 345.67(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 15:54:21.954614: step 6040, G loss = 10350.16, new loss = 448.03(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:54:28.590381: step 6050, G loss = 10187.66, new loss = -135.12(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:54:35.153647: step 6060, G loss = 10859.01, new loss = 122.58(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:54:41.776721: step 6070, G loss = 9978.51, new loss = -679.88(50.8 examples/sec; 0.631 sec/batch)
2018-10-09 15:54:48.366388: step 6080, G loss = 10626.52, new loss = 98.24(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:54:54.984239: step 6090, G loss = 10382.75, new loss = -533.45(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:55:01.586542: step 6100, G loss = 10083.43, new loss = 55.83(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:55:09.040103: step 6110, G loss = 10315.97, new loss = -245.98(44.9 examples/sec; 0.713 sec/batch)
2018-10-09 15:55:15.653285: step 6120, G loss = 10144.01, new loss = -555.61(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:55:22.321062: step 6130, G loss = 9641.84, new loss = -439.47(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 15:55:28.894378: step 6140, G loss = 10259.77, new loss = -290.78(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 15:55:35.449975: step 6150, G loss = 10518.69, new loss = 100.07(51.4 examples/sec; 0.622 sec/batch)
2018-10-09 15:55:42.047745: step 6160, G loss = 10649.67, new loss = 1103.05(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:55:48.618054: step 6170, G loss = 10253.84, new loss = -127.57(51.3 examples/sec; 0.624 sec/batch)
2018-10-09 15:55:55.258531: step 6180, G loss = 10512.15, new loss = 325.26(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:56:01.919245: step 6190, G loss = 10501.21, new loss = 788.50(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:56:08.471818: step 6200, G loss = 10823.18, new loss = 277.29(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:56:15.905854: step 6210, G loss = 10683.48, new loss = 332.35(45.1 examples/sec; 0.710 sec/batch)
2018-10-09 15:56:22.503915: step 6220, G loss = 10615.00, new loss = 167.71(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:56:29.129886: step 6230, G loss = 10305.88, new loss = -109.64(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:56:35.740203: step 6240, G loss = 10226.64, new loss = -507.42(51.0 examples/sec; 0.627 sec/batch)
2018-10-09 15:56:42.371978: step 6250, G loss = 10822.16, new loss = -33.55(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:56:48.988911: step 6260, G loss = 10399.62, new loss = -152.26(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:56:55.675158: step 6270, G loss = 10377.13, new loss = -261.45(50.3 examples/sec; 0.636 sec/batch)
2018-10-09 15:57:02.348123: step 6280, G loss = 10521.56, new loss = 205.80(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 15:57:09.045321: step 6290, G loss = 10252.36, new loss = -146.76(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 15:57:15.685186: step 6300, G loss = 10508.30, new loss = -91.96(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 15:57:23.052837: step 6310, G loss = 10590.61, new loss = 374.53(45.5 examples/sec; 0.704 sec/batch)
2018-10-09 15:57:29.650364: step 6320, G loss = 10277.83, new loss = 15.40(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 15:57:36.264256: step 6330, G loss = 10313.39, new loss = -27.23(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 15:57:42.938244: step 6340, G loss = 10586.14, new loss = -55.26(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 15:57:49.576393: step 6350, G loss = 10219.24, new loss = -516.67(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 15:57:56.246508: step 6360, G loss = 10444.56, new loss = 35.82(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 15:58:02.852691: step 6370, G loss = 10777.77, new loss = -37.70(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:58:09.537094: step 6380, G loss = 10434.05, new loss = 142.21(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 15:58:16.185285: step 6390, G loss = 10803.85, new loss = 132.60(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 15:58:22.897122: step 6400, G loss = 10441.40, new loss = -349.18(50.3 examples/sec; 0.637 sec/batch)
2018-10-09 15:58:31.019218: step 6410, G loss = 10344.89, new loss = -123.05(41.1 examples/sec; 0.778 sec/batch)
2018-10-09 15:58:37.649854: step 6420, G loss = 10191.42, new loss = 454.47(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:58:44.310875: step 6430, G loss = 10035.64, new loss = -261.64(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 15:58:50.890921: step 6440, G loss = 10530.34, new loss = 26.07(51.4 examples/sec; 0.623 sec/batch)
2018-10-09 15:58:57.521744: step 6450, G loss = 9956.19, new loss = -314.31(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:59:04.245991: step 6460, G loss = 10295.62, new loss = -572.65(50.1 examples/sec; 0.639 sec/batch)
2018-10-09 15:59:10.869969: step 6470, G loss = 10314.40, new loss = 16.62(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:59:17.479821: step 6480, G loss = 10378.80, new loss = -9.16(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:59:24.150742: step 6490, G loss = 10153.97, new loss = -17.14(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 15:59:30.804440: step 6500, G loss = 10520.32, new loss = 279.88(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 15:59:38.386049: step 6510, G loss = 10455.76, new loss = -80.90(44.1 examples/sec; 0.726 sec/batch)
2018-10-09 15:59:45.016668: step 6520, G loss = 10281.65, new loss = -580.35(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 15:59:51.625430: step 6530, G loss = 10723.03, new loss = 965.08(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 15:59:58.255330: step 6540, G loss = 10453.99, new loss = 183.74(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 16:00:04.891780: step 6550, G loss = 10089.81, new loss = -468.38(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 16:00:11.486133: step 6560, G loss = 10293.34, new loss = -339.64(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 16:00:18.121803: step 6570, G loss = 10499.96, new loss = -359.25(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 16:00:24.746047: step 6580, G loss = 10652.52, new loss = 821.06(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 16:00:31.486318: step 6590, G loss = 10294.61, new loss = -576.85(49.9 examples/sec; 0.642 sec/batch)
2018-10-09 16:00:38.054886: step 6600, G loss = 10377.81, new loss = -307.36(51.2 examples/sec; 0.625 sec/batch)
2018-10-09 16:00:45.505286: step 6610, G loss = 10083.52, new loss = -499.86(44.9 examples/sec; 0.713 sec/batch)
2018-10-09 16:00:52.122315: step 6620, G loss = 10032.97, new loss = -236.56(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 16:00:58.760884: step 6630, G loss = 10320.60, new loss = -244.46(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 16:01:05.390138: step 6640, G loss = 10120.20, new loss = -447.62(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 16:01:11.964778: step 6650, G loss = 10122.17, new loss = -352.45(51.2 examples/sec; 0.626 sec/batch)
2018-10-09 16:01:18.588853: step 6660, G loss = 10662.56, new loss = -88.86(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 16:01:25.318170: step 6670, G loss = 10468.89, new loss = 425.59(50.0 examples/sec; 0.640 sec/batch)
2018-10-09 16:01:31.923434: step 6680, G loss = 10575.93, new loss = 292.29(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 16:01:38.541313: step 6690, G loss = 10381.19, new loss = -106.06(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 16:01:45.165143: step 6700, G loss = 10579.95, new loss = 152.92(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 16:01:52.550182: step 6710, G loss = 10636.04, new loss = 235.10(45.3 examples/sec; 0.706 sec/batch)
2018-10-09 16:01:59.196355: step 6720, G loss = 9794.43, new loss = -230.18(50.5 examples/sec; 0.633 sec/batch)
2018-10-09 16:02:05.850570: step 6730, G loss = 10230.60, new loss = 95.89(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 16:02:12.462868: step 6740, G loss = 9961.05, new loss = -25.96(50.9 examples/sec; 0.628 sec/batch)
2018-10-09 16:02:19.070439: step 6750, G loss = 10537.09, new loss = 160.81(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 16:02:25.705301: step 6760, G loss = 10264.42, new loss = 721.15(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 16:02:32.314146: step 6770, G loss = 10823.74, new loss = -411.47(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 16:02:39.012947: step 6780, G loss = 10675.15, new loss = 456.57(50.3 examples/sec; 0.637 sec/batch)
2018-10-09 16:02:45.597540: step 6790, G loss = 10457.47, new loss = 187.45(51.2 examples/sec; 0.626 sec/batch)
2018-10-09 16:02:52.242514: step 6800, G loss = 10520.48, new loss = 60.18(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 16:02:59.990571: step 6810, G loss = 10191.24, new loss = -302.84(43.1 examples/sec; 0.742 sec/batch)
2018-10-09 16:03:06.601669: step 6820, G loss = 10588.03, new loss = -43.38(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 16:03:13.185358: step 6830, G loss = 10206.59, new loss = -96.39(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 16:03:19.808858: step 6840, G loss = 10526.11, new loss = -59.55(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 16:03:26.414293: step 6850, G loss = 10365.87, new loss = -214.90(50.9 examples/sec; 0.629 sec/batch)
2018-10-09 16:03:33.093877: step 6860, G loss = 10202.97, new loss = -452.58(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 16:03:39.769032: step 6870, G loss = 9961.60, new loss = -466.12(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 16:03:46.408964: step 6880, G loss = 10225.86, new loss = -194.90(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 16:03:53.054983: step 6890, G loss = 10419.45, new loss = -258.83(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 16:03:59.725031: step 6900, G loss = 10096.74, new loss = -169.86(50.5 examples/sec; 0.634 sec/batch)
2018-10-09 16:04:07.196339: step 6910, G loss = 10813.86, new loss = -135.90(44.9 examples/sec; 0.713 sec/batch)
2018-10-09 16:04:13.831364: step 6920, G loss = 10754.70, new loss = 200.66(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 16:04:20.477724: step 6930, G loss = 10370.61, new loss = -422.31(50.6 examples/sec; 0.632 sec/batch)
2018-10-09 16:04:27.070916: step 6940, G loss = 10394.43, new loss = -97.94(51.1 examples/sec; 0.626 sec/batch)
2018-10-09 16:04:33.706133: step 6950, G loss = 10402.54, new loss = -45.22(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 16:04:40.378344: step 6960, G loss = 10497.59, new loss = -664.02(50.6 examples/sec; 0.633 sec/batch)
2018-10-09 16:04:47.071060: step 6970, G loss = 10405.57, new loss = -537.03(50.2 examples/sec; 0.637 sec/batch)
2018-10-09 16:04:53.783003: step 6980, G loss = 10304.66, new loss = -565.22(50.1 examples/sec; 0.638 sec/batch)
2018-10-09 16:05:00.524710: step 6990, G loss = 10584.39, new loss = 176.59(49.9 examples/sec; 0.641 sec/batch)
2018-10-09 16:05:07.161055: step 7000, G loss = 9925.04, new loss = -424.74(50.7 examples/sec; 0.631 sec/batch)
2018-10-09 16:05:20.006273: step 7010, G loss = 10261.38, new loss = -451.85(25.6 examples/sec; 1.251 sec/batch)
2018-10-09 16:05:26.692429: step 7020, G loss = 10376.86, new loss = 109.32(50.4 examples/sec; 0.635 sec/batch)
2018-10-09 16:05:33.352846: step 7030, G loss = 10623.37, new loss = -124.43(50.8 examples/sec; 0.630 sec/batch)
2018-10-09 16:05:39.952384: step 7040, G loss = 10171.87, new loss = -311.21(51.0 examples/sec; 0.628 sec/batch)
2018-10-09 16:05:46.603282: step 7050, G loss = 10367.45, new loss = -359.40(50.6 examples/sec; 0.632 sec/batch)
