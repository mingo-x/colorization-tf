(u'correspondence', u'0')
(u'g_repeat', u'1')
(u'is_coco', u'1')
(u'restore_opt', u'1')
(u'with_caption', u'0')
(u'd_repeat', u'1')
(u'prior_boost', u'1')
(u'is_gan', u'0')
(u'batch_size', u'32')
(u'gan', u'0')
(u'ckpt', u'/srv/glusterfs/xieya/tf_coco_imgp/models/model.ckpt-4000')
(u'is_rgb', u'0')
(u'image_size', u'224')
(u'gpus', u'0')
(u'gp_lambda', u'10')
(u'k', u'1')
(u'g_version', u'1')
(u'version', u'11')
(u'temp_trainable', u'0')
(u'alpha', u'0.01')
(u'weight_decay', u'0.001')
Discriminator has no correspondence.
Not using GAN.
Using prior boost.
Learning rate G: 4e-06 D: 0.0001
Adversarial weight 0.01
Generator version 1
Discriminator version 11
Gradient penalty 10.0.
Gradient norm 1.0.
Solver initialization done.
Graph constructed.
Session configured.
/srv/glusterfs/xieya/tf_coco_imgp/models/model.ckpt-4000 restored.
Global step: 4000.0
2018-10-09 15:57:44.378997: step 4000, G loss = 10772.21, new loss = 680.51(21.1 examples/sec; 1.517 sec/batch)
2018-10-09 15:58:08.440479: step 4010, G loss = 10577.74, new loss = -173.25(13.6 examples/sec; 2.349 sec/batch)
2018-10-09 15:58:19.788135: step 4020, G loss = 10628.99, new loss = -98.61(29.5 examples/sec; 1.085 sec/batch)
2018-10-09 15:58:30.621371: step 4030, G loss = 10288.65, new loss = -180.48(31.1 examples/sec; 1.030 sec/batch)
2018-10-09 15:58:41.369844: step 4040, G loss = 10316.47, new loss = 146.59(31.3 examples/sec; 1.023 sec/batch)
2018-10-09 15:58:52.155209: step 4050, G loss = 10221.23, new loss = 124.03(31.1 examples/sec; 1.028 sec/batch)
2018-10-09 15:59:03.130839: step 4060, G loss = 10075.97, new loss = -362.49(30.6 examples/sec; 1.045 sec/batch)
2018-10-09 15:59:14.044804: step 4070, G loss = 10528.98, new loss = 88.02(30.8 examples/sec; 1.039 sec/batch)
2018-10-09 15:59:24.974376: step 4080, G loss = 9939.16, new loss = -713.96(30.8 examples/sec; 1.038 sec/batch)
2018-10-09 15:59:35.908694: step 4090, G loss = 10281.71, new loss = -681.87(30.7 examples/sec; 1.041 sec/batch)
2018-10-09 15:59:46.812411: step 4100, G loss = 11081.95, new loss = 829.83(30.8 examples/sec; 1.038 sec/batch)
2018-10-09 15:59:58.952557: step 4110, G loss = 10361.13, new loss = 842.95(27.5 examples/sec; 1.162 sec/batch)
2018-10-09 16:00:09.895892: step 4120, G loss = 10690.04, new loss = 961.87(30.7 examples/sec; 1.043 sec/batch)
2018-10-09 16:00:20.840328: step 4130, G loss = 10056.61, new loss = -602.47(30.7 examples/sec; 1.043 sec/batch)
2018-10-09 16:00:31.676067: step 4140, G loss = 10141.97, new loss = -148.05(31.0 examples/sec; 1.033 sec/batch)
2018-10-09 16:00:42.632368: step 4150, G loss = 10464.13, new loss = 331.85(30.6 examples/sec; 1.044 sec/batch)
2018-10-09 16:00:53.555068: step 4160, G loss = 10535.99, new loss = 370.95(30.8 examples/sec; 1.040 sec/batch)
2018-10-09 16:01:04.606703: step 4170, G loss = 10957.20, new loss = 1193.44(30.5 examples/sec; 1.048 sec/batch)
2018-10-09 16:01:15.114314: step 4180, G loss = 10133.93, new loss = -243.95(31.8 examples/sec; 1.005 sec/batch)
2018-10-09 16:01:25.456400: step 4190, G loss = 10418.50, new loss = -61.03(32.4 examples/sec; 0.987 sec/batch)
2018-10-09 16:01:35.699241: step 4200, G loss = 10007.86, new loss = -534.55(32.7 examples/sec; 0.978 sec/batch)
2018-10-09 16:01:46.083452: step 4210, G loss = 10402.29, new loss = -110.65(32.0 examples/sec; 1.001 sec/batch)
2018-10-09 16:01:55.731495: step 4220, G loss = 10806.94, new loss = 1289.52(34.7 examples/sec; 0.923 sec/batch)
2018-10-09 16:02:05.486354: step 4230, G loss = 10448.09, new loss = -390.20(34.2 examples/sec; 0.937 sec/batch)
2018-10-09 16:02:14.995298: step 4240, G loss = 10486.98, new loss = -595.14(35.1 examples/sec; 0.913 sec/batch)
2018-10-09 16:02:24.502724: step 4250, G loss = 10405.56, new loss = -288.43(35.1 examples/sec; 0.911 sec/batch)
2018-10-09 16:02:34.162805: step 4260, G loss = 10255.14, new loss = -257.76(34.6 examples/sec; 0.925 sec/batch)
2018-10-09 16:02:44.368723: step 4270, G loss = 10414.63, new loss = -128.35(32.7 examples/sec; 0.979 sec/batch)
2018-10-09 16:02:54.551230: step 4280, G loss = 10636.94, new loss = 941.39(32.8 examples/sec; 0.977 sec/batch)
2018-10-09 16:03:04.731171: step 4290, G loss = 10474.33, new loss = 531.21(32.8 examples/sec; 0.977 sec/batch)
2018-10-09 16:03:14.871799: step 4300, G loss = 10499.45, new loss = -557.38(32.9 examples/sec; 0.973 sec/batch)
2018-10-09 16:03:25.821748: step 4310, G loss = 10842.24, new loss = 699.89(30.3 examples/sec; 1.055 sec/batch)
2018-10-09 16:03:35.566804: step 4320, G loss = 10978.98, new loss = 815.61(34.2 examples/sec; 0.934 sec/batch)
2018-10-09 16:03:45.488213: step 4330, G loss = 10753.76, new loss = 488.02(33.6 examples/sec; 0.951 sec/batch)
2018-10-09 16:03:55.412951: step 4340, G loss = 10505.68, new loss = 23.09(33.6 examples/sec; 0.952 sec/batch)
2018-10-09 16:04:05.524300: step 4350, G loss = 10613.92, new loss = -81.91(33.0 examples/sec; 0.969 sec/batch)
2018-10-09 16:04:15.676107: step 4360, G loss = 10073.99, new loss = -491.96(32.9 examples/sec; 0.974 sec/batch)
2018-10-09 16:04:25.760220: step 4370, G loss = 10611.38, new loss = 87.13(33.1 examples/sec; 0.968 sec/batch)
2018-10-09 16:04:35.811819: step 4380, G loss = 10674.76, new loss = -291.78(33.2 examples/sec; 0.964 sec/batch)
2018-10-09 16:04:45.958144: step 4390, G loss = 10730.00, new loss = -228.20(32.9 examples/sec; 0.973 sec/batch)
2018-10-09 16:04:56.139815: step 4400, G loss = 10706.84, new loss = 361.79(32.7 examples/sec; 0.977 sec/batch)
2018-10-09 16:05:07.310819: step 4410, G loss = 10543.53, new loss = -139.62(29.7 examples/sec; 1.076 sec/batch)
2018-10-09 16:05:17.415608: step 4420, G loss = 10347.41, new loss = 180.91(33.0 examples/sec; 0.970 sec/batch)
2018-10-09 16:05:27.515236: step 4430, G loss = 10166.43, new loss = -193.33(33.1 examples/sec; 0.968 sec/batch)
2018-10-09 16:05:37.603182: step 4440, G loss = 10170.72, new loss = -258.10(33.1 examples/sec; 0.967 sec/batch)
2018-10-09 16:05:47.691475: step 4450, G loss = 10301.84, new loss = -232.05(33.1 examples/sec; 0.968 sec/batch)
2018-10-09 16:05:58.007869: step 4460, G loss = 10074.63, new loss = -68.61(32.5 examples/sec; 0.984 sec/batch)
2018-10-09 16:06:09.097254: step 4470, G loss = 10332.18, new loss = -180.47(30.2 examples/sec; 1.059 sec/batch)
